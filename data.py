# -*- coding: utf-8 -*-
"""data1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rCpqLyEF2wRimL56AGL5hoAi_tH_vYMM
"""

!pip install streamlit
!pip install xhtml2pdf

import streamlit as st
import os
import multiprocessing
from bs4 import BeautifulSoup
import urllib.request
from xhtml2pdf import pisa
import shutil
from pyngrok import ngrok


def convert_to_pdf(input_file, output_folder_path):

    _, file_extension = os.path.splitext(input_file)
    val = file_extension.lower()
    if val == '.mhtml' or val == '.html' or val == '.htm' or val == '.igs':
            url = f"file:///{os.path.abspath(input_file)}"

            # Download the content of the file
            response = urllib.request.urlopen(url)

            # Create a BeautifulSoup object to parse the content
            soup = BeautifulSoup(response.read(), "html.parser")

            # Find the elements you want to scrape
            data = soup.find_all()

            # Convert the scraped data to a PDF
            output_file_name = os.path.splitext(os.path.basename(input_file))[0] + ".pdf"
            output_file_path = os.path.join(output_folder_path, output_file_name)
            with open(output_file_path, "wb") as f:
                html = "<html><head><meta charset='UTF-8'></head><body>"
                for d in data:
                    html += str(d)
                html += "</body></html>"
                pisa.CreatePDF(html, dest=f)
            print(f"File converted: {output_file_path}")
            response.close()
            os.remove(input_file)

    elif val == '.pdf':
        output_file_name = os.path.basename(input_file)
        output_file_path = os.path.join(output_folder_path, output_file_name)
        shutil.copyfile(input_file, output_file_path)
        os.remove(input_file)


def scrape_files(input_folder_path, output_folder_path):



    # Get a list of all files in the input folder
    input_files = [os.path.join(input_folder_path, f) for f in os.listdir(input_folder_path) if
                   os.path.isfile(os.path.join(input_folder_path, f))]


    # Create a process pool and convert each file to PDF using a separate process
    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
        results = []
        for input_file in input_files:
            results.append(pool.apply_async(convert_to_pdf, args=(input_file, output_folder_path)))
        for r in results:
            r.wait()




# Streamlit UI code
def main():
  st.title("File Scraping and Conversion to PDF")

  # Get user inputs
  input_folder_path = st.text_input("Input Folder Path", "/path/to/input/folder")
  output_folder_path = st.text_input("Output Folder Path", "/path/to/output/folder")

  if st.button("Scrape and Convert"):
    st.text("Scraping and converting files...")
    scrape_files(input_folder_path, output_folder_path)
    st.text("File scraping and conversion completed.")


# Run the Streamlit application
if __name__ == "__main__":
  ngrok_tunnel = ngrok.connect(8501)
  print('Public URL:', ngrok_tunnel.public_url)
  main()

!mkdir -p ~/.streamlit/

!echo "server.port = $PORT" >> ~/.streamlit/config.toml

!echo "[server]\nheadless = true\nenableCORS=false\n" > ~/.streamlit/config.toml

!pip install streamlit pyngrok